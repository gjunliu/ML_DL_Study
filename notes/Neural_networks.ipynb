{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Aritificial Neurons\n",
        "A **single neuron** in a deep neural network is the basic computational unit. It takes in one or more inputs, applies weights to them, sums them up, adds a bias, and passes the result through an **activation function**.\n",
        "\n",
        "## What a Single Neuron Does:\n",
        "Mathematically:\n",
        "\n",
        "\n",
        "$$y = \\sigma\\left( \\sum_{i=1}^{n} w_i x_i + b \\right)$$\n",
        "\n",
        "where:\n",
        "- $x_i$ = input values\n",
        "- $w_i$ = weights\n",
        "- $b$ = bias\n",
        "- $\\sigma$ = activation function (e.g. Sigmoid, ReLU)\n",
        "- $y$ = output of the neuron"
      ],
      "metadata": {
        "id": "cWRgBFWa5AJ_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Single Neuron in Python"
      ],
      "metadata": {
        "id": "aUNrGCer9EGg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Inputs, weights, and bias\n",
        "inputs = np.array([0.5, 0.8])\n",
        "weights = np.array([0.4, 0.7])\n",
        "bias = -0.1\n",
        "\n",
        "# Sigmoid activation\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# ReLU activation\n",
        "def relu(x):\n",
        "    return max(0, x)\n",
        "\n",
        "\n",
        "# Neuron output\n",
        "\n",
        "# # Approach 1: Wighted sum\n",
        "# z = sum(w * x for w, x in zip(weights, inputs)) + bias\n",
        "\n",
        "# Approach 2: using numpy dot product function\n",
        "z = np.dot(weights, inputs) + bias\n",
        "\n",
        "output = sigmoid(z)\n",
        "# output = relu(z)"
      ],
      "metadata": {
        "id": "TLV-fJnt53Oj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Understand **Inputs**, **Weights** and **Bias**\n",
        "\n",
        "## Input Values($x$)\n",
        "There are the **features** or **signals** that feed into the neuron.\n",
        "- In an image: each pixel value (e.g., brightness) is an input.\n",
        "  - For example, a grayscale image of size 28×28 has 784 inputs (one for each pixel).\n",
        "- In text: each word or token embedding might be an input.\n",
        "- In tabular data: each column (feature) is an input.\n",
        "\n",
        "## Weights($w$)\n",
        "Weights determine the **importance** of each input.\n",
        "- A larger weight means the neuron pays more attention to that input.\n",
        "- A weight of zero means the neuron ignores that input.\n",
        "\n",
        "## Bias($b$)\n",
        "The bias allows the neuron to **shift** the activation function left or right.\n",
        "- Without bias, the neuron would always output zero when all inputs are zero.\n",
        "- Bias increases the model’s flexibility and ability to fit data.\n",
        "\n",
        "## Why they are often high-dimensional?\n",
        "**Real-world data is complex:** images, audio, video, or text involve many features.\n",
        "- A color image (224×224×3) has over 150,000 input values.\n",
        "- A 100-word sentence with 300-dim embeddings has 30,000 inputs.\n",
        "\n",
        "Each input feature needs a corresponding weight, so if inputs are high-dimensional, weights are too.\n",
        "\n",
        "In deep learning, high dimensionality enables the model to capture complex patterns, but it also requires careful regularization to avoid overfitting.\n",
        "\n",
        "## Intuition\n",
        "Think of a single neuron like this:\n",
        "\n",
        "- Inputs = What the neuron senses\n",
        "- Weights = What the neuron cares about\n",
        "- Bias = The neuron's sensitivity threshold\n",
        "- Activation = The neuron’s decision to act or not"
      ],
      "metadata": {
        "id": "4q3z3r4C_u5g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Single Neuron Diagram**\n",
        "\n",
        "<pre>\n",
        "Input Features →    Weights     →     Sum + Bias     →   Activation   →   Output\n",
        "  [x₁, x₂, x₃]        [w₁, w₂, w₃]        + b               f(z)             y\n",
        "      │                   │                 │                 │              │\n",
        "      └── x₁ × w₁ ──┐     └──── x₂ × w₂ ─┐  │                 ↓              ↓\n",
        "                    ├───────────────► z = w·x + b ───► f(z) ────────► Output y\n",
        "      └── x₃ × w₃ ──┘                                (e.g., ReLU, sigmoid)\n",
        "</pre>\n",
        "\n"
      ],
      "metadata": {
        "id": "ldWGENcsD_bI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **From Neuron to Layer**\n",
        "\n",
        "When you have **multiple neurons** in a layer, each neuron independently performs 'single neuron diagram' computation but with its own weights and bias:\n",
        "\n",
        "         [x₁, x₂, x₃] → Layer\n",
        "               ↓\n",
        "       ┌──────────────┬──────────────┬──────────────┐\n",
        "       │  Neuron 1    │  Neuron 2    │  Neuron 3    │\n",
        "       │ w₁·x + b₁    │ w₂·x + b₂    │ w₃·x + b₃    │\n",
        "       └──────┬───────┴──────┬───────┴──────┬───────┘\n",
        "              ↓              ↓              ↓\n",
        "         Output y₁       Output y₂       Output y₃\n",
        "\n",
        "This gives a vector output (e.g., [y1, y2, y3]) — which can be passed to the next layer in a deep network.\n",
        "\n",
        "\n",
        "## Example: Real image input (e.g., 28x28 MNIST Digit)\n",
        "- Input: 784-dimensional vector (28×28 grayscale pixels flattened)\n",
        "- Hidden layer: 128 neurons → 128 different weight vectors (each size 784)\n",
        "- Output: 10 neurons (for digits 0–9)\n",
        "\n",
        "Each layer transforms data into a more **abstract representation**, leading to classification, detection, or prediction at the output."
      ],
      "metadata": {
        "id": "xSr5fC8VN3aw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def relu(x):\n",
        "  return np.maximum(0, x)\n",
        "\n",
        "def sigmoid(x):\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# input: suppose 3 features\n",
        "x = np.array([0.6, 0.9, 0.1])\n",
        "\n",
        "# Single Neuron (random weights + bias)\n",
        "w = np.random.randn(3)\n",
        "b = np.random.randn()\n",
        "\n",
        "# Forward Pass through single neuron\n",
        "z = np.dot(w, x) + b\n",
        "output = relu(z)\n",
        "print(\"Single Neuron Output (ReLU):\", output)\n",
        "\n",
        "# A layer with 5 neurons\n",
        "# Simulate a layer with 5 neurons\n",
        "W = np.random.randn(5, 3)  # 5 neurons, each with 3 input weights\n",
        "b = np.random.randn(5)     # 5 biases (one per neuron)\n",
        "\n",
        "# Forward Pass\n",
        "z = np.dot(W, x) + b\n",
        "layer_output = relu(z)\n",
        "print(\"Layer Output (ReLU):\", layer_output)\n",
        "\n",
        "# Plot the output values of the neurons in the layer\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.bar(range(1, 6), layer_output)\n",
        "plt.xlabel('Neuron Index')\n",
        "plt.ylabel('Activation (ReLU)')\n",
        "plt.title('Activations from a 5-Neuron Layer')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "RDQUQX-MRuwG",
        "outputId": "1c327eee-3ec1-483d-8a41-95865f5ef495"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Single Neuron Output (ReLU): 0.4243057147656587\n",
            "Layer Output (ReLU): [0.         1.76778288 0.4947008  0.         1.96819568]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extended to 2-layer Feedforward Neural Network"
      ],
      "metadata": {
        "id": "TstWfBrbU6zu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------\n",
        "# Layer 1: 5 neurons (ReLU)\n",
        "# -------------------------------\n",
        "W1 = np.random.randn(5, 3)   # shape: (neurons, input_dim)\n",
        "b1 = np.random.randn(5)      # one bias per neuron\n",
        "\n",
        "z1 = np.dot(W1, x) + b1\n",
        "a1 = relu(z1)  # activation output from layer 1\n",
        "\n",
        "# -------------------------------\n",
        "# Layer 2: 2 neurons (Sigmoid)\n",
        "# -------------------------------\n",
        "W2 = np.random.randn(2, 5)   # shape: (neurons, previous_layer_dim)\n",
        "b2 = np.random.randn(2)\n",
        "\n",
        "z2 = np.dot(W2, a1) + b2\n",
        "a2 = sigmoid(z2)  # final output\n",
        "\n",
        "# Print outputs\n",
        "print(\"Layer 1 Output (ReLU):\", a1)\n",
        "print(\"Layer 2 Output (Sigmoid):\", a2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_asiIsQVDxf",
        "outputId": "76d4250f-321b-480b-f821-6c06055b39f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer 1 Output (ReLU): [0.52763855 0.33515013 0.         0.         0.77305119]\n",
            "Layer 2 Output (Sigmoid): [0.37358924 0.22343856]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Why Use Different Numbers of Neurons per Layer?**\n",
        "\n",
        "1. Dimensionality reduction and abstraction\n",
        "- As you go deeper into the network, the model abstracts features — from low-level (e.g. edges) to high-level (e.g. faces).\n",
        "- Fewer neurons in deeper layers can force the model to compress knowledge, encouraging generalization.\n",
        "  - e.x: Image input (784 neurons) → Hidden layer (256) → Hidden (128) → Output (10 classes)\n",
        "\n",
        "2. Feature Specialization\n",
        "- Wide early layers let the network capture many low-level patterns.\n",
        "- Later layers become more specialized and often need fewer neurons to combine features effectively.\n",
        "\n",
        "3. Regularization by Design\n",
        "- Using fewer neurons in deeper layers helps prevent overfitting by limiting model capacity.\n",
        "- It also reduces computational cost.\n",
        "\n",
        "## Common Heuristics\n",
        "\n",
        "1. Pyramid rule:\n",
        "Use fewer neurons as you go deeper:\n",
        "Input → 512 → 256 → 128 → 64 → Output\n",
        "\n",
        "2. Empirical testing:\n",
        "Try multiple setups and compare via validation accuracy and loss.\n",
        "\n",
        "3. Power-of-2 rule:\n",
        "Try neuron counts like 32, 64, 128, 256… These work well with hardware and are common starting points.\n",
        "\n",
        "4. Avoid over-parameterization:\n",
        "Don't use more neurons than necessary — you’ll overfit and waste compute.\n",
        "\n",
        "## **Dont' Forget These:**\n",
        "- **Activation functions** and **normalization** also affect performance (not just neuron count).\n",
        "\n",
        "- **Depth vs width tradeoff:** More neurons can sometimes replace deeper layers, but deeper nets often generalize better.\n",
        "\n",
        "- **Hyperparameter tuning** (with cross-validation) is essential to find the best configuration."
      ],
      "metadata": {
        "id": "OLl-HPEHXU6c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculate total number of Weights in the Neural Network\n",
        "\n",
        "## Given:\n",
        "- Input dimension: 10\n",
        "- 1st hidden layer: 20 neurons\n",
        "- 2nd hidden layer: 30 neurons\n",
        "- 3rd hidden layer: 40 neurons\n",
        "- Output layer: 1 neuron\n",
        "\n",
        "## Weight Calculatin(including biases)\n",
        "1. Input → Layer 1 (10 inputs → 20 neurons):\n",
        "- Each neuron has 10 weights + 1 bias → 11 parameters\n",
        "- Total: (10+1)×20=220 weights\n",
        "\n",
        "2. Layer 1 → Layer 2 (20 inputs → 30 neurons):\n",
        "- Each neuron has 20 weights + 1 bias → 21 parameters\n",
        "- Total: (20+1)×30=630 weights\n",
        "\n",
        "3. Layer 2 → Layer 3 (30 inputs → 40 neurons):\n",
        "- Each neuron has 30 weights + 1 bias → 31 parameters\n",
        "- Total: (30+1)×40=1240 weights\n",
        "\n",
        "4. Layer 3 → Output (40 inputs → 1 neuron):\n",
        "- 1 neuron has 40 weights + 1 bias → 41 parameters\n",
        "- Total: 40+1=41 weights\n",
        "\n",
        "==> **Total number of weights:**\n",
        "  220+630+1240+41= 2131 total weights\n",
        "​\n"
      ],
      "metadata": {
        "id": "6ZB9zWyO4vWO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Activation Functions**\n",
        "An **activation function** introduces non-linearity into the output of a neuron. Without activation functions, a neural network would be equivalent to a linear model, no matter how many layers it has. Non-linear activation functions allow deep networks to learn complex patterns and representations.\n",
        "\n",
        "## Why is it important?\n",
        "- Helps the network learn non-linear mappings from inputs to outputs.\n",
        "- Enables gradient-based optimization (e.g., backpropagation).\n",
        "- Allows stacking multiple layers effectively (i.e., depth becomes meaningful).\n",
        "\n",
        "# **Why No activation = Linear Model**\n",
        "Without activation functions, each layer in a neural network only applies a **linear transformation**:\n",
        "$$y=W⋅x+b$$\n",
        "If you stack multiple such linear layers **without** non-linearity, the result is still a single linear transformation — because the **composition of linear functions is still linear.**\n",
        "\n",
        "##Mathematically:\n",
        "Suppose you have two layers:\n",
        "- First layer:\n",
        "$$h=W_1 x + b$$\n",
        "- Second layer:\n",
        "$$\n",
        "y = W_2 h + b_2 = W_2(W_1 x + b_1) + b_2 = (W_2 W_1)x + (W_2 b_1 + b_2)\n",
        "$$\n",
        "\n",
        "This is **just another linear transformation** of input $x$.\n",
        "\n",
        "###Simple Numerical Example\n",
        "Let's take a small neural network with:\n",
        "- Input $x = [1, 2]$\n",
        "- First layer: weights $W_1 = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4\\\\ \\end{bmatrix}$, bias $b_1 = [1, 1]$\n",
        "- Second layer: weights $W_2 = [1 1]$, bias $b_2 = 0$\n",
        "\n",
        "Step 1: First layer (linear)\n",
        "$$\n",
        "h = W_1 x + b_1 = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4\\\\ \\end{bmatrix} \\begin{bmatrix} 1  \\\\ 2\\\\ \\end{bmatrix} + \\begin{bmatrix} 1 \\\\ 1\\end{bmatrix} = \\begin{bmatrix} 1 + 4 + 1 \\\\ 3 + 8 + 1\\end{bmatrix} = \\begin{bmatrix} 6 \\\\ 12\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Step 2: Second layer (linear)\n",
        "$$\n",
        "y = W_2 h + b_2 = \\begin{bmatrix} 1 & 1\\end{bmatrix}\\begin{bmatrix} 6 \\\\ 12\\end{bmatrix} = 6 + 12 = 18\n",
        "$$\n",
        "\n",
        "This is just a linear transformation of the original input. No matter how many such layers you add, it won’t model non-linear patterns unless you insert an activation function between layers.\n",
        "\n",
        "## Why this is a problem\n",
        "A linear model can only learn **linear relationships**. That means:\n",
        "- It cannot capture complex patterns.\n",
        "- It cannot separate classes that are not linearly separable.\n",
        "\n",
        "In contrast, non-linear activation functions like ReLU or Sigmoid **break this linearity**, allowing the network to learn powerful non-linear mappings."
      ],
      "metadata": {
        "id": "03NtDYzXtrnD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Common Activation Functions**\n",
        "| Activation                            | Formula                                                       | Properties                         | Use Case                                    |\n",
        "|---------------------------------------|----------------------------------------------------------------|------------------------------------|---------------------------------------------|\n",
        "| **ReLU** (Rectified Linear Unit)      | $f(x) = \\max(0, x)$                                            | Fast, sparsity, avoids saturation  | Default for hidden layers                   |\n",
        "| **Sigmoid**                           | $f(x) = \\frac{1}{1 + e^{-x}}$                                  | Smooth, outputs [0, 1]             | Output layer for binary classification      |\n",
        "| **Tanh**                              | $f(x) = \\tanh(x)$                                              | Outputs [-1, 1], zero-centered     | Rare today, sometimes in RNNs               |\n",
        "| **Leaky ReLU**                        | $f(x) = \\max(\\alpha x, x)$, $\\alpha \\approx 0.01$              | Solves ReLU dying neuron problem   | Hidden layers when ReLU fails               |\n",
        "| **ELU** (Exponential Linear Unit)     | Smooth version of ReLU                                         | Improved convergence in some cases | Deep networks                               |\n",
        "| **Softmax**                           | $f_i(x) = \\frac{e^{x_i}}{\\sum_j e^{x_j}}$                      | Outputs probability distribution   | Output layer for multi-class classification |\n",
        "| **Swish** (by Google)                 | $f(x) = x \\cdot \\text{sigmoid}(x)$                             | Self-gated, smooth                 | In some modern models like EfficientNet     |\n",
        "| **GELU** (Gaussian Error Linear Unit) | Approx: $f(x) = 0.5x(1 + \\tanh(\\sqrt{2/\\pi}(x + 0.0447x^3)))$  | Used in transformers               | BERT, GPT, etc.                             |\n"
      ],
      "metadata": {
        "id": "U3NYju4G_5d6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How to pick a suitable activation function\n",
        "| Situation                          | Recommended Function                                              |\n",
        "| ---------------------------------- | ----------------------------------------------------------------- |\n",
        "| Hidden layers in most networks     | **ReLU** (or Leaky ReLU if dying ReLU issue)                      |\n",
        "| Binary classification output       | **Sigmoid**                                                       |\n",
        "| Multi-class classification output  | **Softmax**                                                       |\n",
        "| RNNs / time series models          | Historically **Tanh** or **Sigmoid**, now often **ReLU** variants |\n",
        "| Transformer-based models           | **GELU**                                                          |\n",
        "| When experimenting for performance | Try **Swish** or **ELU**                                          |\n"
      ],
      "metadata": {
        "id": "rwMhBc8LAJeI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tips**\n",
        "- Always use **non-linear activation** between layers.\n",
        "- Avoid **Sigmoid** in deep hidden layers due to vanishing gradients.\n",
        "- Choose based on **task, layer position, and architecture type**."
      ],
      "metadata": {
        "id": "e8bp5lW5Atjf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Fully Connected Layer (Dense Layer)**\n",
        "A fully connected layer (also called a dense layer) is a layer where every neuron is connected to every neuron in the previous layer.\n",
        "\n",
        "## **Key Characteristics:**\n",
        "- Each connection has a **weight** and a **bias**.\n",
        "- It performs a **linear transformation**, followed by an **activation function**.\n",
        "\n",
        "## Mathematically:\n",
        "$$\n",
        "z = W \\cdot x + b \\\\\n",
        "a = f(z)\n",
        "$$\n",
        "\n",
        "## Purpose\n",
        "- Used in hidden layers to learn complex features and patterns from data.\n",
        "- Often appears after convolution or pooling layers in CNNs."
      ],
      "metadata": {
        "id": "13H_txDVBRB6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Output Layer**\n",
        "The output layer is the **final layer** in a neural network. It generates the **final prediction** or result of the model.\n",
        "\n",
        "## Characteristic\n",
        "- Still fully connected in most cases.\n",
        "- Activation function is task-specific:\n",
        "  - **Classification (binary)** → Sigmoid\n",
        "  - **Classification (multi-class)** → Softmax\n",
        "  - Regression → Linear (or none)"
      ],
      "metadata": {
        "id": "xO2QifMCEiUT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Fully Connected Layer vs. Output layer\n",
        "| Feature     | Fully Connected Layer                | Output Layer                       |\n",
        "| ----------- | ------------------------------------ | ---------------------------------- |\n",
        "| Position    | Hidden layers (usually middle)       | Final layer                        |\n",
        "| Connections | Every input connects to every output | Same (fully connected)             |\n",
        "| Activation  | ReLU, Tanh, etc.                     | Depends on task (Sigmoid, Softmax) |\n",
        "| Purpose     | Learn features                       | Generate final prediction          |\n"
      ],
      "metadata": {
        "id": "bi3BmFjiFg_C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Loss Function**\n",
        "A loss function (also called an objective function or cost function) computes a numerical value (loss) that quantifies the difference between the predicted output and the true label.\n",
        "\n",
        "$$ Loss = Loss(y_{true}, y_{pred})$$\n",
        "\n",
        "- Lower loss = better predictions\n",
        "- Higher loss = worse predictions\n",
        "\n",
        "In deep learning, the loss function is a critical component that **measures how well a model's predictions match the actual target values.** It tells the network how wrong it is, and guides the training process to improve performance.\n",
        "\n",
        "## Role of Loss function in deep learning\n",
        "1. Guides learning\n",
        "  - It acts as a signal to adjust the model’s weights using backpropagation and gradient descent.\n",
        "  - Without a loss function, the model has no objective to optimize.\n",
        "2. Enables optimization\n",
        "  - Gradients (partial derivatives of the loss with respect to model weights) are calculated and used to update weights.\n",
        "  - The goal is to minimize the loss over the training data.\n",
        "3. Evaluates performance(during training)\n",
        "  - Used in training and validation phases to monitor model improvement.\n",
        "  - Helps detect underfitting or overfitting.\n",
        "\n",
        "\n",
        "## **Common Loss Functions**\n",
        "| Problem Type                   | Loss Function                        | Formula / Description                                    |                                     |   |\n",
        "| ------------------------------ | ------------------------------------ | -------------------------------------------------------- | ----------------------------------- | - |\n",
        "| **Regression**                 | **Mean Squared Error (MSE)**         | $\\frac{1}{n} \\sum (y_{\\text{true}} - y_{\\text{pred}})^2$ |                                     |   |\n",
        "|                                | **Mean Absolute Error (MAE)**        | $\\frac{1}{n} \\sum |y\\_{\\text{true}} - y\\_{\\text{pred}} |$ |\n",
        "| **Binary Classification**      | **Binary Cross-Entropy**             | $-[y \\log(p) + (1 - y)\\log(1 - p)]$                      |                                     |   |\n",
        "| **Multi-Class Classification** | **Categorical Cross-Entropy**        | $-\\sum y_i \\log(p_i)$                                    |                                     |   |\n",
        "| **Multi-Label**                | **Binary Cross-Entropy (per label)** | Used when multiple labels apply                          |                                     |   |\n",
        "\n",
        "\n",
        "## Example: Binary Classification\n",
        "Suppose:\n",
        "\n",
        "- True label $y = 1$\n",
        "- Model prediction $\\hat y = 0.9$\n",
        "\n",
        "Then Binary corss-entropy loss is:\n",
        "$$\n",
        "Loss = -[1 ⋅ log(0.9) + 0 ⋅ log(1 - 0.9] = 0.105\n",
        "$$\n",
        "If prediction was worse $\\hat y = 0.1$, the loss is:\n",
        "$$\n",
        "Loss = -log(0.1) = 2.30 (much worse)\n",
        "$$\n"
      ],
      "metadata": {
        "id": "L7QUnB3yG5r9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Cross-Entropy**\n",
        "Cross-entropy tells us how well the predicted probability distribution matches the actual labels. If the prediction is perfect, cross-entropy is low; if it's wrong, cross-entropy is high.\n",
        "\n",
        "## Mathematical definition\n",
        "### For binary classification:\n",
        "$$\n",
        "Loss = -[y ⋅ \\log(\\hat y) + (1 - y)\\log(1 - \\hat y)]\n",
        "$$\n",
        "where:\n",
        "- $ y \\in {0, 1}$: true label\n",
        "- $ \\hat y \\in {0, 1}$: predicted probability\n",
        "\n",
        "### For multi-class classification(Softmax)\n",
        "If there are **C classes** and the true label vector is one-hot encoded:\n",
        "$$\n",
        "Loss = - \\sum_{i=1}^{C} y_{i} ⋅ \\log(\\hat y_{i})\n",
        "$$\n",
        "Where:\n",
        "- $y_i = 1$ if class $i$ is correct, else 0\n",
        "- $\\hat y_i = $ predicted probability for class $i$\n",
        "since only one class is correct, only one term contributes to the loss.\n",
        "\n",
        "## Why use cross-entropy?\n",
        "- It is mathematically grounded in information theory.\n",
        "- Encourages the model to assign higher probabilities to the correct class.\n",
        "- Differentiable → suitable for gradient-based optimization.\n",
        "\n",
        "## **When to use**\n",
        "| Task                                      | Use This                         |\n",
        "| ----------------------------------------- | -------------------------------- |\n",
        "| Binary classification                     | Binary cross-entropy             |\n",
        "| Multi-class classification (single label) | Categorical cross-entropy        |\n",
        "| Multi-label classification                | Binary cross-entropy (per label) |\n",
        "\n"
      ],
      "metadata": {
        "id": "wVSLeLWySnQf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Entropy in Informatin Theory**\n",
        "In information theory, entropy is a measure of uncertainty, surprise, or information content in a random variable. It was introduced by Claude Shannon in 1948 and is fundamental to fields like data compression, communication, cryptography, and machine learning.\n",
        "\n",
        "## What is Entropy?\n",
        "Entropy quantifies how unpredictable or informative a probability distribution is.\n",
        "\n",
        "### Mathematically:\n",
        "If $X$ is a discrete random variable with possible outcomes $x_1, x_2, ..., x_n$ and corresponding probabilities $p(x_1), p(x_2), ..., p(x_n)$, then the **Shannon entropy** is:\n",
        "$$\n",
        "H(X) = -\\sum_{i=1}^{n}p(x_i)\\log_{2}p(x_i)\n",
        "$$\n",
        "\n",
        "- Unit: bits (when using base 2 logarithm)\n",
        "- High entropy → more uncertainty / randomness\n",
        "- Low entropy → more certainty / predictability\n",
        "\n",
        "\n",
        "## Intuition with examples:\n",
        "1. Fair coin toss:\n",
        "- $p(H) = 0.5$, $p(T) = 0.5$\n",
        "- $H = -[0.5\\log_{2}(0.5) + 0.5\\log_{2}(0.5)] = -[-0.5 - 0.5] = 1$\n",
        "- perfect uncertainty -> entropy is maximal\n",
        "\n",
        "2. Biased Coin Toss:\n",
        "- $p(H) = 0.99$, $p(T) = 0.01$\n",
        "- $H ≈ 0.08$ bits -> low entropy (almost always heads)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eisldHa3WpGt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Backpropagation**\n",
        "\n",
        "Backpropagation is the **mechanism for computing gradients** of the loss with respect to each weight in the network, using the chain rule of calculus. These gradients are then used by an **optimizer** (like SGD or Adam) to update the weights.\n",
        "\n",
        "Backpropagation (short for backward propagation of errors) is the fundamental algorithm used to train deep neural networks by adjusting weights to minimize the loss function. **It’s how a neural network learns from labeled data.**\n",
        "\n",
        "## High level steps:\n",
        "1. **Forward pass:** Compute outputs layer by layer and calculate the loss.\n",
        "2. **Backward pass:** Use the chain rule to compute gradients of the loss w.r.t. each parameter.\n",
        "3. **Weight update:** Adjust parameters using gradient descent (or a variant)."
      ],
      "metadata": {
        "id": "zb9eyPJ-pmmp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple Intuitve Example for Backpropagation\n",
        "## Init setup\n",
        "- Input: 2 values\n",
        "- Hidden Layer 1: 2 neurons\n",
        "- Hidden Layer 2: 2 neurons\n",
        "- ReLU activation for both hidden layers\n",
        "- Sigmoid activation for the output layer\n",
        "- Binary cross-entropy loss\n",
        "\n",
        "### Inputs:\n",
        "- $x_1 = 1.0$, $x_2 = 2.0$\n",
        "- Ground-truth label: $y = 1.0$\n",
        "\n",
        "### Weights and biases:\n",
        "Layer 1:\n",
        "  - w11 = 0.1, w12 = 0.2, b1 = 0.0 (for neuron h11)\n",
        "  - w13 = 0.3, w14 = 0.4, b2 = 0.0 (for neuron h12)\n",
        "\n",
        "Layer 2:\n",
        "  - w21 = 0.5, w22 = 0.6, b3 = 0.0 (for neuron h21)\n",
        "  - w23 = 0.7, w24 = 0.8, b4 = 0.0 (for neuron h22)\n",
        "\n",
        "Output layer:\n",
        "  - w31 = 0.9, w32 = 1.0, b5 = 0.0\n",
        "\n",
        "\n",
        "## Step 1: **Forward pass**\n",
        "\n",
        "**Layer 1 (hidden)**:\n",
        "<pre>\n",
        "z11 = w11*x1 + w12*x2 + b1 = 0.1*1 + 0.2*2 = 0.5\n",
        "a11 = ReLU(z11) = 0.5\n",
        "\n",
        "z12 = w13*x1 + w14*x2 + b2 = 0.3*1 + 0.4*2 = 1.1\n",
        "a12 = ReLU(z12) = 1.1\n",
        "</pre>\n",
        "\n",
        "**Layer 2 (hidden)**\n",
        "Input = [0.5, 1.1], the output from layer1\n",
        "\n",
        "<pre>\n",
        "z21 = w21*a11 + w22*a12 + b3 = 0.5*0.5 + 0.6*1.1 = 0.25 + 0.66 = 0.91\n",
        "a21 = ReLU(z21) = 0.91\n",
        "\n",
        "z22 = w23*a11 + w24*a12 + b4 = 0.7*0.5 + 0.8*1.1 = 0.35 + 0.88 = 1.23\n",
        "a22 = ReLU(z22) = 1.23\n",
        "</pre>\n",
        "\n",
        "**Output Layer:**\n",
        "Input = [0.91, 1.23], the output from layer2\n",
        "\n",
        "<pre>\n",
        "z3 = w31*a21 + w32*a22 + b5 = 0.9*0.91 + 1.0*1.23 = 0.819 + 1.23 = 2.049\n",
        "ŷ = sigmoid(z3) = 1 / (1 + exp(-2.049)) ≈ 0.8858\n",
        "</pre>\n",
        "\n",
        "## Step 2: **Compute Loss** (Binary Cross Entropy)\n",
        "$$\n",
        "L = -[y \\cdot \\log(\\hat y) + (1 - y) \\cdot \\log(1 - \\hat y)] \\\\\n",
        "L = -[log(0.8858)] ≈ 0.121\n",
        "$$\n",
        "\n",
        "## Step 3: **Backward Pass**\n",
        "\n",
        "**Output Layer**\n",
        "**Gradient of loss w.r.t output:**\n",
        "\n",
        "Since\n",
        "$L = -[y \\cdot \\log(\\hat y) + (1 - y) \\cdot \\log(1 - \\hat y)]$ \\\\\n",
        "and\n",
        "$y = 1$ in this case, so\n",
        "$$\n",
        "\\frac{\\partial{L}}{∂{\\hat y}} = -[\\frac{y}{\\hat y} - \\frac{1 - y}{1 - \\hat y}]\n",
        "$$\n",
        "\n",
        "**Derivative of sigmoid:**\n",
        "Since $\\sigma(x) = \\frac{1}{1 + exp(-x)}$, and\n",
        "\\begin{aligned}\n",
        "\\frac{d}{dx}\\sigma(x) &= \\frac{d}{dx}[\\frac{1}{1 + exp(-x)}] \\\\\n",
        "&= \\frac{d}{dx}(1 + e^{-x})^{-1} \\\\\n",
        "&= -(1+e^{-x})^{-2}(-e^{-x}) \\\\\n",
        "&= \\frac{e_{-x}}{(1 + e^{-x})^2} \\\\\n",
        "&= \\frac{1}{1 + e^{-x}} \\cdot \\frac{e^{-x}}{1 + e^{-x}} \\\\\n",
        "&= \\frac{1}{1 + e^{-x}} \\cdot (1 - \\frac{1}{1 + e^{-x}}) \\\\\n",
        "&= σ(x) \\cdot (1 - σ(x)) \\\\\n",
        "\\end{aligned}\n",
        "\n",
        "Thus:\n",
        "$$\n",
        "\\frac{\\partial\\hat y}{∂z3} = \\hat y (1 - \\hat y)\n",
        "$$\n",
        "\n",
        "**Chain rule:**\n",
        "$$\n",
        "\\frac{\\partial L}{∂z3} = \\frac{∂L}{∂\\hat y} \\cdot \\frac{∂\\hat y}{∂z3} = -[\\frac{y}{\\hat y} - \\frac{1 - y}{1 - \\hat y}] \\cdot \\hat y (1 - \\hat y) = \\hat y - y = 0.8858 - 1 = -0.1142\n",
        "$$\n",
        "**Special cases**: if you are using sigmoid activation + BCE loss, then\n",
        "$\\frac{\\partial L}{∂z} = \\hat y - y$, this is a well-known simplification in deep learning.\n",
        "\n",
        "**Gradients for weights in output layer:**\n",
        "$$\n",
        "\\frac{∂L}{∂w31} = \\frac{∂L}{∂z3} \\cdot a21 ≈ -0.1142 \\cdot \\mathbf{0.91} ≈ -0.1039 \\\\\n",
        "\\frac{∂L}{∂w32} = \\frac{∂L}{∂z3} \\cdot a22 = -0.1142 \\cdot \\mathbf{1.23} ≈ -0.1405\n",
        "$$\n",
        "\n",
        "**Hiden Layer 2**\n",
        "$$\n",
        "\\frac{∂L}{∂a21} = \\frac{∂L}{∂\\hat y} \\cdot \\frac{∂\\hat y}{∂z3} \\cdot \\frac{∂z3}{∂a21} = -0.1142 ⋅ w31 = -0.1142 \\cdot 0.9 =  -0.1028\n",
        "$$\n",
        "ReLU's at z21 = 1 -> 1, so:\n",
        "$$\n",
        "\\frac{∂L}{∂z21} = -0.1028 \\\\\n",
        "\\frac{∂L}{∂w21} = \\frac{∂L}{∂z21} \\cdot a11 = -0.1028 \\cdot \\mathbf{0.5} = -0.0514 \\\\\n",
        "\\frac{∂L}{∂w22} = \\frac{∂L}{∂z21} \\cdot a12  = -0.1028 \\cdot \\mathbf{1.1} = -0.1131\n",
        "$$\n",
        "\n",
        "\\\\\n",
        "\n",
        "$$\n",
        "\\frac{∂L}{∂a22} = \\frac{∂L}{∂\\hat y} \\cdot \\frac{∂\\hat y}{∂z3} \\cdot \\frac{∂z3}{∂a22} = -0.1142 ⋅ w32 = -0.1142 \\cdot 1.0 =  -0.1142\n",
        "$$\n",
        "ReLU's at z22 = 1 -> 1, so:\n",
        "$$\n",
        "\\frac{∂L}{∂z22} = -0.1142 \\\\\n",
        "\\frac{∂L}{∂w23} = \\frac{∂L}{∂z22} \\cdot a11 = -0.142 \\cdot \\mathbf{0.5} = -0.0571 \\\\\n",
        "\\frac{∂L}{∂w24} = \\frac{∂L}{∂z22} \\cdot a12  = -0.142 \\cdot \\mathbf{1.1} = -0.1256\n",
        "$$\n",
        "\n",
        "**Hiden Layer 1**\n",
        "\n",
        "Follow the same chain rule:\n",
        "\\begin{aligned}\n",
        "\\frac{∂L}{∂a11} &= \\frac{∂L}{∂\\hat y} \\cdot \\frac{∂\\hat y}{∂z3} \\cdot \\frac{∂z3}{∂a21} + \\frac{∂L}{∂\\hat y} \\cdot \\frac{∂\\hat y}{∂z3} \\cdot \\frac{∂z3}{∂a22} \\\\\n",
        "&= \\frac{∂L}{∂z21} \\cdot w21 + \\frac{∂L}{∂z22} \\cdot w23 \\\\\n",
        "&=-0.1028 ⋅ 0.5 + (-0.1142) \\cdot 0.7 \\\\\n",
        "&=-0.1313 \\\\\n",
        "\\end{aligned}\n",
        "\n",
        "\\begin{aligned}\n",
        "\\frac{∂L}{∂a12} &= \\frac{∂L}{∂\\hat y} \\cdot \\frac{∂\\hat y}{∂z3} \\cdot \\frac{∂z3}{∂a21} + \\frac{∂L}{∂\\hat y} \\cdot \\frac{∂\\hat y}{∂z3} \\cdot \\frac{∂z3}{∂a22} \\\\\n",
        "&= \\frac{∂L}{∂z21} \\cdot w22 + \\frac{∂L}{∂z22} \\cdot w24 \\\\\n",
        "&=-0.1028 \\cdot 0.6 + (-0.1142) \\cdot 0.8 \\\\\n",
        "&=-0.1531\n",
        "\\end{aligned}\n",
        "\n",
        "Then:\n",
        "$$\n",
        "\\frac{∂L}{∂w11} = \\frac{∂L}{∂a11} \\cdot ReLU'(z11)\n",
        "$$\n",
        "ReLU's at z11 = 0.5 -> 1, so:\n",
        "$$\n",
        "\\frac{∂L}{∂w11} = \\frac{∂L}{∂z11} \\cdot x1 = \\frac{∂L}{∂a11} \\cdot x1 = -0.1313 \\cdot 1 = -0.1313\\\\\n",
        "\\frac{∂L}{∂w12} = \\frac{∂L}{∂z11} \\cdot x2 = \\frac{∂L}{∂a11} \\cdot x2 = -0.1313 \\cdot 2 = -0.2626\\\\\n",
        "\\frac{∂L}{∂w13} = \\frac{∂L}{∂z12} \\cdot x1 = \\frac{∂L}{∂a12} \\cdot x1 =-0.1531 \\cdot 1 = -0.1531\\\\\n",
        "\\frac{∂L}{∂w14} = \\frac{∂L}{∂z12} \\cdot x2 = \\frac{∂L}{∂a12} \\cdot x2 =-0.1531 \\cdot 2 = -0.3062\\\\\n",
        "$$"
      ],
      "metadata": {
        "id": "fygM76lisFt3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Weight Updates**\n",
        "With learning rate $\\eta = 0.1$:\n",
        "\n",
        "$w^{new} = w_{old} - \\eta * \\frac{∂L}{∂w_{old}}$"
      ],
      "metadata": {
        "id": "tIQ8F0jvhIY1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code Implementation"
      ],
      "metadata": {
        "id": "S4c3GstilwB9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define activation functions and their derivatives\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_derivative(x):\n",
        "    # Ensure input is a NumPy array for consistent behavior\n",
        "    x = np.array(x)\n",
        "    return (x > 0).astype(float)\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    s = sigmoid(x)\n",
        "    return s * (1 - s)\n",
        "\n",
        "# Define Binary Cross-Entrop Loss and its Derivative\n",
        "def binary_cross_entropy(y_true, y_pred):\n",
        "    epsilon = 1e-10 # Avoid log(0)\n",
        "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
        "    return -(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
        "\n",
        "def binary_cross_entropy_derivative(y_true, y_pred):\n",
        "    # Derivative of BCE with respect to y_pred\n",
        "    epsilon = 1e-10\n",
        "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
        "    return (y_pred - y_true) / (y_pred * (1 - y_pred))"
      ],
      "metadata": {
        "id": "iD7rvznWkNNM"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Neural Network Setup\n",
        "# Inputs\n",
        "x1 = 1.0\n",
        "x2 = 2.0\n",
        "X = np.array([x1, x2])\n",
        "\n",
        "# Ground-truth label\n",
        "y_true = 1.0\n",
        "\n",
        "# Weights and biases\n",
        "# Layer 1\n",
        "w11, w12, b1 = 0.1, 0.2, 0.0  # Neuron h11\n",
        "w13, w14, b2 = 0.3, 0.4, 0.0  # Neuron h12\n",
        "\n",
        "# Layer 2\n",
        "w21, w22, b3 = 0.5, 0.6, 0.0  # Neuron h21\n",
        "w23, w24, b4 = 0.7, 0.8, 0.0  # Neuron h22\n",
        "\n",
        "# Output layer\n",
        "w31, w32, b5 = 0.9, 1.0, 0.0\n",
        "\n",
        "# Learning rate\n",
        "learning_rate = 0.1 # A common choice, adjust as needed\n",
        "\n",
        "print(\"--- Initial Weights and Biases ---\")\n",
        "print(f\"Layer 1: w11={w11}, w12={w12}, b1={b1}\")\n",
        "print(f\"         w13={w13}, w14={w14}, b2={b2}\")\n",
        "print(f\"Layer 2: w21={w21}, w22={w22}, b3={b3}\")\n",
        "print(f\"         w23={w23}, w24={w24}, b4={b4}\")\n",
        "print(f\"Output Layer: w31={w31}, w32={w32}, b5={b5}\")\n",
        "print(f\"Learning Rate: {learning_rate}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCfVnrmFl4D1",
        "outputId": "23098fa7-3caf-4c66-bc32-f783f7d447b6"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Initial Weights and Biases ---\n",
            "Layer 1: w11=0.1, w12=0.2, b1=0.0\n",
            "         w13=0.3, w14=0.4, b2=0.0\n",
            "Layer 2: w21=0.5, w22=0.6, b3=0.0\n",
            "         w23=0.7, w24=0.8, b4=0.0\n",
            "Output Layer: w31=0.9, w32=1.0, b5=0.0\n",
            "Learning Rate: 0.1\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Forward Pass**"
      ],
      "metadata": {
        "id": "AQwqHvpPmUCM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- Forward Pass ---\")\n",
        "\n",
        "# Layer 1 Calculations\n",
        "# Neuron h11\n",
        "z11 = w11 * x1 + w12 * x2 + b1\n",
        "h11 = relu(z11)\n",
        "print(f\"z11 (w11*x1 + w12*x2 + b1): {w11}*{x1} + {w12}*{x2} + {b1} = {z11}\")\n",
        "print(f\"h11 (ReLU(z11)): {h11}\")\n",
        "\n",
        "# Neuron h12\n",
        "z12 = w13 * x1 + w14 * x2 + b2\n",
        "h12 = relu(z12)\n",
        "print(f\"z12 (w13*x1 + w14*x2 + b2): {w13}*{x1} + {w14}*{x2} + {b2} = {z12}\")\n",
        "print(f\"h12 (ReLU(z12)): {h12}\\n\")\n",
        "\n",
        "# Store outputs of Layer 1 for Layer 2 inputs\n",
        "H1 = np.array([h11, h12])\n",
        "\n",
        "# Layer 2 Calculations\n",
        "# Neuron h21\n",
        "z21 = w21 * h11 + w22 * h12 + b3\n",
        "h21 = relu(z21)\n",
        "print(f\"z21 (w21*h11 + w22*h12 + b3): {w21}*{h11} + {w22}*{h12} + {b3} = {z21}\")\n",
        "print(f\"h21 (ReLU(z21)): {h21}\")\n",
        "\n",
        "# Neuron h22\n",
        "z22 = w23 * h11 + w24 * h12 + b4\n",
        "h22 = relu(z22)\n",
        "print(f\"z22 (w23*h11 + w24*h12 + b4): {w23}*{h11} + {w24}*{h12} + {b4} = {z22}\")\n",
        "print(f\"h22 (ReLU(z22)): {h22}\\n\")\n",
        "\n",
        "# Store outputs of Layer 2 for Output Layer inputs\n",
        "H2 = np.array([h21, h22])\n",
        "\n",
        "# Output Layer Calculation\n",
        "z_out = w31 * h21 + w32 * h22 + b5\n",
        "y_pred = sigmoid(z_out)\n",
        "print(f\"z_out (w31*h21 + w32*h22 + b5): {w31}*{h21} + {w32}*{h22} + {b5} = {z_out}\")\n",
        "print(f\"y_pred (Sigmoid(z_out)): {y_pred}\\n\")\n",
        "\n",
        "# Calculate Initial Loss\n",
        "loss = binary_cross_entropy(y_true, y_pred)\n",
        "print(f\"Ground Truth (y_true): {y_true}\")\n",
        "print(f\"Predicted Output (y_pred): {y_pred}\")\n",
        "print(f\"Initial Loss (Binary Cross-Entropy): {loss}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O2LFxVJ5mKe2",
        "outputId": "9b3dfd2b-78c2-4c3e-bf00-d256221408e1"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Forward Pass ---\n",
            "z11 (w11*x1 + w12*x2 + b1): 0.1*1.0 + 0.2*2.0 + 0.0 = 0.5\n",
            "h11 (ReLU(z11)): 0.5\n",
            "z12 (w13*x1 + w14*x2 + b2): 0.3*1.0 + 0.4*2.0 + 0.0 = 1.1\n",
            "h12 (ReLU(z12)): 1.1\n",
            "\n",
            "z21 (w21*h11 + w22*h12 + b3): 0.5*0.5 + 0.6*1.1 + 0.0 = 0.91\n",
            "h21 (ReLU(z21)): 0.91\n",
            "z22 (w23*h11 + w24*h12 + b4): 0.7*0.5 + 0.8*1.1 + 0.0 = 1.23\n",
            "h22 (ReLU(z22)): 1.23\n",
            "\n",
            "z_out (w31*h21 + w32*h22 + b5): 0.9*0.91 + 1.0*1.23 + 0.0 = 2.049\n",
            "y_pred (Sigmoid(z_out)): 0.8858465352801156\n",
            "\n",
            "Ground Truth (y_true): 1.0\n",
            "Predicted Output (y_pred): 0.8858465352801156\n",
            "Initial Loss (Binary Cross-Entropy): 0.12121155412456014\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Backpropagation**"
      ],
      "metadata": {
        "id": "LGofOeXXm5Pp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dL/dy_pred\n",
        "dL_dy_pred = binary_cross_entropy_derivative(y_true, y_pred)\n",
        "print(f\"dL/dy_pred (derivative of BCE wrt y_pred): {dL_dy_pred}\")\n",
        "\n",
        "# dy_pred/dz_out\n",
        "dy_pred_dz_out = sigmoid_derivative(z_out)\n",
        "print(f\"dy_pred/dz_out (derivative of Sigmoid wrt z_out): {dy_pred_dz_out}\")\n",
        "\n",
        "# dL/dz_out (Chain rule: dL/dy_pred * dy_pred/dz_out)\n",
        "dL_dz_out = dL_dy_pred * dy_pred_dz_out\n",
        "print(f\"dL/dz_out (dL/dy_pred * dy_pred/dz_out): {dL_dz_out}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OwdAx380ms17",
        "outputId": "9343f35d-483d-4da6-97ae-66357e6ce016"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dL/dy_pred (derivative of BCE wrt y_pred): -1.128863702880305\n",
            "dy_pred/dz_out (derivative of Sigmoid wrt z_out): 0.10112245121233052\n",
            "dL/dz_out (dL/dy_pred * dy_pred/dz_out): -0.11415346471988441\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Gradients for weights and bias of the Output Layer**"
      ],
      "metadata": {
        "id": "QbDSdRs9nl7I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dL/dw31 = dL/dz_out * dz_out/dw31 = dL/dz_out * h21\n",
        "dL_dw31 = dL_dz_out * h21\n",
        "print(f\"dL/dw31 (dL/dz_out * h21): {dL_dw31}\")\n",
        "\n",
        "# dL/dw32 = dL/dz_out * dz_out/dw32 = dL/dz_out * h22\n",
        "dL_dw32 = dL_dz_out * h22\n",
        "print(f\"dL/dw32 (dL/dz_out * h22): {dL_dw32}\")\n",
        "\n",
        "# dL/db5 = dL/dz_out * dz_out/db5 = dL/dz_out * 1\n",
        "dL_db5 = dL_dz_out * 1\n",
        "print(f\"dL/db5 (dL/dz_out * 1): {dL_db5}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NTI5Ir-knbbr",
        "outputId": "39aa86d8-4358-4894-ccaa-2c5432e9979a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dL/dw31 (dL/dz_out * h21): -0.10387965289509482\n",
            "dL/dw32 (dL/dz_out * h22): -0.14040876160545782\n",
            "dL/db5 (dL/dz_out * 1): -0.11415346471988441\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Gradients for Hidden Layer 2**"
      ],
      "metadata": {
        "id": "CD5WUrCgn9bQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We need to propagate error back from the output layer to hidden layer 2.\n",
        "# The error signal that neuron h21 receives is dL/dh21.\n",
        "# The error signal that neuron h22 receives is dL/dh22.\n",
        "\n",
        "# dL/dh21 = dL/dz_out * dz_out/dh21 = dL/dz_out * w31\n",
        "dL_dh21 = dL_dz_out * w31\n",
        "print(f\"dL/dh21 (dL/dz_out * w31): {dL_dh21}\")\n",
        "\n",
        "# dL/dh22 = dL/dz_out * dz_out/dh22 = dL/dz_out * w32\n",
        "dL_dh22 = dL_dz_out * w32\n",
        "print(f\"dL/dh22 (dL/dz_out * w32): {dL_dh22}\\n\")\n",
        "\n",
        "# dL/dz21 = dL/dh21 * dh21/dz21 = dL/dh21 * ReLU_derivative(z21)\n",
        "dL_dz21 = dL_dh21 * relu_derivative(z21)\n",
        "print(f\"dL/dz21 (dL/dh21 * ReLU_derivative(z21)): {dL_dz21}\")\n",
        "\n",
        "# dL/dz22 = dL/dh22 * dh22/dz22 = dL/dh22 * ReLU_derivative(z22)\n",
        "dL_dz22 = dL_dh22 * relu_derivative(z22)\n",
        "print(f\"dL/dz22 (dL/dh22 * ReLU_derivative(z22)): {dL_dz22}\\n\")\n",
        "\n",
        "# Gradients for weights and biases of Hidden Layer 2\n",
        "# dL/dw21 = dL/dz21 * dz21/dw21 = dL/dz21 * h11\n",
        "dL_dw21 = dL_dz21 * h11\n",
        "print(f\"dL/dw21 (dL/dz21 * h11): {dL_dw21}\")\n",
        "\n",
        "# dL/dw22 = dL/dz21 * dz21/dw22 = dL/dz21 * h12\n",
        "dL_dw22 = dL_dz21 * h12\n",
        "print(f\"dL/dw22 (dL/dz21 * h12): {dL_dw22}\")\n",
        "\n",
        "# dL/db3 = dL/dz21 * dz21/db3 = dL/dz21 * 1\n",
        "dL_db3 = dL_dz21 * 1\n",
        "print(f\"dL/db3 (dL/dz21 * 1): {dL_db3}\\n\")\n",
        "\n",
        "# dL/dw23 = dL/dz22 * dz22/dw23 = dL/dz22 * h11\n",
        "dL_dw23 = dL_dz22 * h11\n",
        "print(f\"dL/dw23 (dL/dz22 * h11): {dL_dw23}\")\n",
        "\n",
        "# dL/dw24 = dL/dz22 * dz22/dw24 = dL/dz22 * h12\n",
        "dL_dw24 = dL_dz22 * h12\n",
        "print(f\"dL/dw24 (dL/dz22 * h12): {dL_dw24}\")\n",
        "\n",
        "# dL/db4 = dL/dz22 * dz22/db4 = dL/dz22 * 1\n",
        "dL_db4 = dL_dz22 * 1\n",
        "print(f\"dL/db4 (dL/dz22 * 1): {dL_db4}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Es4E7HXoDxl",
        "outputId": "74dfec6d-50e1-440a-b999-7261bf48ae33"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dL/dh21 (dL/dz_out * w31): -0.10273811824789597\n",
            "dL/dh22 (dL/dz_out * w32): -0.11415346471988441\n",
            "\n",
            "dL/dz21 (dL/dh21 * ReLU_derivative(z21)): -0.10273811824789597\n",
            "dL/dz22 (dL/dh22 * ReLU_derivative(z22)): -0.11415346471988441\n",
            "\n",
            "dL/dw21 (dL/dz21 * h11): -0.051369059123947985\n",
            "dL/dw22 (dL/dz21 * h12): -0.11301193007268558\n",
            "dL/db3 (dL/dz21 * 1): -0.10273811824789597\n",
            "\n",
            "dL/dw23 (dL/dz22 * h11): -0.057076732359942206\n",
            "dL/dw24 (dL/dz22 * h12): -0.12556881119187285\n",
            "dL/db4 (dL/dz22 * 1): -0.11415346471988441\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Gradients for Hidden Layer 1**"
      ],
      "metadata": {
        "id": "3my8xkciqXu3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A hidden neuron in Layer 1 (e.g., h11) contributes to both h21 and h22.\n",
        "# So, dL/dh11 = (dL/dz21 * dz21/dh11) + (dL/dz22 * dz22/dh11)\n",
        "# dL/dh11 = (dL/dz21 * w21) + (dL/dz22 * w23)\n",
        "dL_dh11 = (dL_dz21 * w21) + (dL_dz22 * w23)\n",
        "print(f\"dL/dh11 (sum of dL/dz21*w21 and dL/dz22*w23): {dL_dh11}\")\n",
        "\n",
        "# dL/dh12 = (dL/dz21 * dz21/dh12) + (dL/dz22 * dz22/dh12)\n",
        "# dL/dh12 = (dL/dz21 * w22) + (dL/dz22 * w24)\n",
        "dL_dh12 = (dL_dz21 * w22) + (dL_dz22 * w24)\n",
        "print(f\"dL/dh12 (sum of dL/dz21*w22 and dL/dz22*w24): {dL_dh12}\\n\")\n",
        "\n",
        "# Now, we need dL/dz11 and dL/dz12\n",
        "# dL/dz11 = dL/dh11 * dh11/dz11 = dL/dh11 * ReLU_derivative(z11)\n",
        "# Pass z11 as a NumPy array\n",
        "dL_dz11 = dL_dh11 * relu_derivative(np.array(z11))\n",
        "print(f\"dL/dz11 (dL/dh11 * ReLU_derivative(z11)): {dL_dz11}\")\n",
        "\n",
        "# dL/dz12 = dL/dh12 * dh12/dz12 = dL/dh12 * ReLU_derivative(z12)\n",
        "# Pass z12 as a NumPy array\n",
        "dL_dz12 = dL_dh12 * relu_derivative(np.array(z12))\n",
        "print(f\"dL/dz12 (dL/dh12 * ReLU_derivative(z12)): {dL_dz12}\\n\")\n",
        "\n",
        "\n",
        "# Gradients for weights and biases of Hidden Layer 1\n",
        "# dL/dw11 = dL/dz11 * dz11/dw11 = dL/dz11 * x1\n",
        "dL_dw11 = dL_dz11 * x1\n",
        "print(f\"dL/dw11 (dL/dz11 * x1): {dL_dw11}\")\n",
        "\n",
        "# dL/dw12 = dL/dz11 * dz11/dw12 = dL/dz11 * x2\n",
        "dL_dw12 = dL_dz11 * x2\n",
        "print(f\"dL/dw12 (dL/dz11 * x2): {dL_dw12}\")\n",
        "\n",
        "# dL/db1 = dL/dz11 * dz11/db1 = dL/dz11 * 1\n",
        "dL_db1 = dL_dz11 * 1\n",
        "print(f\"dL/db1 (dL/dz11 * 1): {dL_db1}\\n\")\n",
        "\n",
        "# dL/dw13 = dL/dz12 * dz12/dw13 = dL/dz12 * x1\n",
        "dL_dw13 = dL_dz12 * x1\n",
        "print(f\"dL/dw13 (dL/dz12 * x1): {dL_dw13}\")\n",
        "\n",
        "# dL/dw14 = dL/dz12 * dz12/dw14 = dL/dz12 * x2\n",
        "dL_dw14 = dL_dz12 * x2\n",
        "print(f\"dL/dw14 (dL/dz12 * x2): {dL_dw14}\")\n",
        "\n",
        "# dL/db2 = dL/dz12 * dz12/db2 = dL/dz12 * 1\n",
        "dL_db2 = dL_dz12 * 1\n",
        "print(f\"dL/db2 (dL/dz12 * 1): {dL_db2}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Krv_q3l3qdSz",
        "outputId": "486d8ec5-9ab9-4ca3-a773-b881ca00a170"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dL/dh11 (sum of dL/dz21*w21 and dL/dz22*w23): -0.13127648442786707\n",
            "dL/dh12 (sum of dL/dz21*w22 and dL/dz22*w24): -0.1529656427246451\n",
            "\n",
            "dL/dz11 (dL/dh11 * ReLU_derivative(z11)): -0.13127648442786707\n",
            "dL/dz12 (dL/dh12 * ReLU_derivative(z12)): -0.1529656427246451\n",
            "\n",
            "dL/dw11 (dL/dz11 * x1): -0.13127648442786707\n",
            "dL/dw12 (dL/dz11 * x2): -0.26255296885573415\n",
            "dL/db1 (dL/dz11 * 1): -0.13127648442786707\n",
            "\n",
            "dL/dw13 (dL/dz12 * x1): -0.1529656427246451\n",
            "dL/dw14 (dL/dz12 * x2): -0.3059312854492902\n",
            "dL/db2 (dL/dz12 * 1): -0.1529656427246451\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Weight Updates**\n",
        "\n",
        "Weight Updates = Current Weights - Learning Rate * Gradient"
      ],
      "metadata": {
        "id": "BbToV5CXvkXR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Update Output Layer weights and bias\n",
        "w31_new = w31 - learning_rate * dL_dw31\n",
        "w32_new = w32 - learning_rate * dL_dw32\n",
        "b5_new = b5 - learning_rate * dL_db5\n",
        "print(f\"New w31: {w31} - {learning_rate} * {dL_dw31} = {w31_new}\")\n",
        "print(f\"New w32: {w32} - {learning_rate} * {dL_dw32} = {w32_new}\")\n",
        "print(f\"New b5: {b5} - {learning_rate} * {dL_db5} = {b5_new}\\n\")\n",
        "\n",
        "# Update Layer 2 weights and biases\n",
        "w21_new = w21 - learning_rate * dL_dw21\n",
        "w22_new = w22 - learning_rate * dL_dw22\n",
        "b3_new = b3 - learning_rate * dL_db3\n",
        "print(f\"New w21: {w21} - {learning_rate} * {dL_dw21} = {w21_new}\")\n",
        "print(f\"New w22: {w22} - {learning_rate} * {dL_dw22} = {w22_new}\")\n",
        "print(f\"New b3: {b3} - {learning_rate} * {dL_db3} = {b3_new}\\n\")\n",
        "\n",
        "w23_new = w23 - learning_rate * dL_dw23\n",
        "w24_new = w24 - learning_rate * dL_dw24\n",
        "b4_new = b4 - learning_rate * dL_db4\n",
        "print(f\"New w23: {w23} - {learning_rate} * {dL_dw23} = {w23_new}\")\n",
        "print(f\"New w24: {w24} - {learning_rate} * {dL_dw24} = {w24_new}\")\n",
        "print(f\"New b4: {b4} - {learning_rate} * {dL_db4} = {b4_new}\\n\")\n",
        "\n",
        "# Update Layer 1 weights and biases\n",
        "w11_new = w11 - learning_rate * dL_dw11\n",
        "w12_new = w12 - learning_rate * dL_dw12\n",
        "b1_new = b1 - learning_rate * dL_db1\n",
        "print(f\"New w11: {w11} - {learning_rate} * {dL_dw11} = {w11_new}\")\n",
        "print(f\"New w12: {w12} - {learning_rate} * {dL_dw12} = {w12_new}\")\n",
        "print(f\"New b1: {b1} - {learning_rate} * {dL_db1} = {b1_new}\\n\")\n",
        "\n",
        "w13_new = w13 - learning_rate * dL_dw13\n",
        "w14_new = w14 - learning_rate * dL_dw14\n",
        "b2_new = b2 - learning_rate * dL_db2\n",
        "print(f\"New w13: {w13} - {learning_rate} * {dL_dw13} = {w13_new}\")\n",
        "print(f\"New w14: {w14} - {learning_rate} * {dL_dw14} = {w14_new}\")\n",
        "print(f\"New b2: {b2} - {learning_rate} * {dL_db2} = {b2_new}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RlmVr9-rvwzH",
        "outputId": "2e3bd592-913e-4cad-c917-8d2cd3f3e394"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New w31: 0.9 - 0.1 * -0.10387965289509482 = 0.9103879652895095\n",
            "New w32: 1.0 - 0.1 * -0.14040876160545782 = 1.0140408761605457\n",
            "New b5: 0.0 - 0.1 * -0.11415346471988441 = 0.011415346471988441\n",
            "\n",
            "New w21: 0.5 - 0.1 * -0.051369059123947985 = 0.5051369059123948\n",
            "New w22: 0.6 - 0.1 * -0.11301193007268558 = 0.6113011930072685\n",
            "New b3: 0.0 - 0.1 * -0.10273811824789597 = 0.010273811824789598\n",
            "\n",
            "New w23: 0.7 - 0.1 * -0.057076732359942206 = 0.7057076732359941\n",
            "New w24: 0.8 - 0.1 * -0.12556881119187285 = 0.8125568811191873\n",
            "New b4: 0.0 - 0.1 * -0.11415346471988441 = 0.011415346471988441\n",
            "\n",
            "New w11: 0.1 - 0.1 * -0.13127648442786707 = 0.11312764844278672\n",
            "New w12: 0.2 - 0.1 * -0.26255296885573415 = 0.22625529688557344\n",
            "New b1: 0.0 - 0.1 * -0.13127648442786707 = 0.013127648442786709\n",
            "\n",
            "New w13: 0.3 - 0.1 * -0.1529656427246451 = 0.3152965642724645\n",
            "New w14: 0.4 - 0.1 * -0.3059312854492902 = 0.43059312854492904\n",
            "New b2: 0.0 - 0.1 * -0.1529656427246451 = 0.015296564272464511\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Implement the Sample Neural Network in TensorFlow**"
      ],
      "metadata": {
        "id": "ypkYJozT8itN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Inputs\n",
        "x1_val = 1.0\n",
        "x2_val = 2.0\n",
        "X = tf.constant([[x1_val, x2_val]], dtype=tf.float32)\n",
        "\n",
        "# Ground-truth label\n",
        "y_true = tf.constant([[1.0]], dtype=tf.float32)\n",
        "\n",
        "# Initial Weights and Biases (as TensorFlow Variables so they can be updated)\n",
        "# Layer 1\n",
        "w11 = tf.Variable(0.1, dtype=tf.float32, name='w11')\n",
        "w12 = tf.Variable(0.2, dtype=tf.float32, name='w12')\n",
        "b1 = tf.Variable(0.0, dtype=tf.float32, name='b1')\n",
        "\n",
        "w13 = tf.Variable(0.3, dtype=tf.float32, name='w13')\n",
        "w14 = tf.Variable(0.4, dtype=tf.float32, name='w14')\n",
        "b2 = tf.Variable(0.0, dtype=tf.float32, name='b2')\n",
        "\n",
        "# Layer 2\n",
        "w21 = tf.Variable(0.5, dtype=tf.float32, name='w21')\n",
        "w22 = tf.Variable(0.6, dtype=tf.float32, name='w22')\n",
        "b3 = tf.Variable(0.0, dtype=tf.float32, name='b3')\n",
        "\n",
        "w23 = tf.Variable(0.7, dtype=tf.float32, name='w23')\n",
        "w24 = tf.Variable(0.8, dtype=tf.float32, name='w24')\n",
        "b4 = tf.Variable(0.0, dtype=tf.float32, name='b4')\n",
        "\n",
        "# Output layer\n",
        "w31 = tf.Variable(0.9, dtype=tf.float32, name='w31')\n",
        "w32 = tf.Variable(1.0, dtype=tf.float32, name='w32')\n",
        "b5 = tf.Variable(0.0, dtype=tf.float32, name='b5')\n",
        "\n",
        "# Group weights and biases by layer for easier management\n",
        "layer1_weights = tf.stack([[w11, w13], [w12, w14]]) # Shape (2, 2)\n",
        "layer1_biases = tf.stack([b1, b2]) # Shape (2,)\n",
        "\n",
        "layer2_weights = tf.stack([[w21, w23], [w22, w24]]) # Shape (2, 2)\n",
        "layer2_biases = tf.stack([b3, b4]) # Shape (2,)\n",
        "\n",
        "output_weights = tf.stack([[w31], [w32]]) # Shape (2, 1)\n",
        "output_bias = b5 # Scalar\n",
        "\n",
        "# List of all trainable variables\n",
        "trainable_variables = [\n",
        "    w11, w12, b1, w13, w14, b2,\n",
        "    w21, w22, b3, w23, w24, b4,\n",
        "    w31, w32, b5\n",
        "]\n",
        "\n",
        "learning_rate = 0.1\n",
        "\n",
        "print(\"--- Initial Weights and Biases (TensorFlow Variables) ---\")\n",
        "for var in trainable_variables:\n",
        "    print(f\"{var.name}: {var.numpy()}\")\n",
        "print(f\"Learning Rate: {learning_rate}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HivWVRW582HT",
        "outputId": "2d1e3bf1-55cc-4853-f4ee-88989f9cbbe5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Initial Weights and Biases (TensorFlow Variables) ---\n",
            "w11:0: 0.10000000149011612\n",
            "w12:0: 0.20000000298023224\n",
            "b1:0: 0.0\n",
            "w13:0: 0.30000001192092896\n",
            "w14:0: 0.4000000059604645\n",
            "b2:0: 0.0\n",
            "w21:0: 0.5\n",
            "w22:0: 0.6000000238418579\n",
            "b3:0: 0.0\n",
            "w23:0: 0.699999988079071\n",
            "w24:0: 0.800000011920929\n",
            "b4:0: 0.0\n",
            "w31:0: 0.8999999761581421\n",
            "w32:0: 1.0\n",
            "b5:0: 0.0\n",
            "Learning Rate: 0.1\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with tf.GradientTape() as tape:\n",
        "    # Forward Pass\n",
        "\n",
        "    # Layer 1\n",
        "    # x_val is [1.0, 2.0]\n",
        "    # x_val[0] = 1.0, x_val[1] = 2.0\n",
        "    # h11 = relu(w11*x1 + w12*x2 + b1)\n",
        "    # h12 = relu(w13*x1 + w14*x2 + b2)\n",
        "    z1_h11 = w11 * X[0, 0] + w12 * X[0, 1] + b1\n",
        "    h11 = tf.nn.relu(z1_h11)\n",
        "\n",
        "    z1_h12 = w13 * X[0, 0] + w14 * X[0, 1] + b2\n",
        "    h12 = tf.nn.relu(z1_h12)\n",
        "    H1 = tf.stack([h11, h12]) # Shape (2,)\n",
        "\n",
        "    print(\"--- Forward Pass (TensorFlow) ---\")\n",
        "    print(f\"z11 (w11*x1 + w12*x2 + b1): {z1_h11.numpy()}\")\n",
        "    print(f\"h11 (ReLU(z11)): {h11.numpy()}\")\n",
        "    print(f\"z12 (w13*x1 + w14*x2 + b2): {z1_h12.numpy()}\")\n",
        "    print(f\"h12 (ReLU(z12)): {h12.numpy()}\\n\")\n",
        "\n",
        "    # Layer 2\n",
        "    # h21 = relu(w21*h11 + w22*h12 + b3)\n",
        "    # h22 = relu(w23*h11 + w24*h12 + b4)\n",
        "    z2_h21 = w21 * H1[0] + w22 * H1[1] + b3\n",
        "    h21 = tf.nn.relu(z2_h21)\n",
        "\n",
        "    z2_h22 = w23 * H1[0] + w24 * H1[1] + b4\n",
        "    h22 = tf.nn.relu(z2_h22)\n",
        "    H2 = tf.stack([h21, h22]) # Shape (2,)\n",
        "\n",
        "    print(f\"z21 (w21*h11 + w22*h12 + b3): {z2_h21.numpy()}\")\n",
        "    print(f\"h21 (ReLU(z21)): {h21.numpy()}\")\n",
        "    print(f\"z22 (w23*h11 + w24*h12 + b4): {z2_h22.numpy()}\")\n",
        "    print(f\"h22 (ReLU(z22)): {h22.numpy()}\\n\")\n",
        "\n",
        "    # Output Layer\n",
        "    # y_pred = sigmoid(w31*h21 + w32*h22 + b5)\n",
        "    z_out = w31 * H2[0] + w32 * H2[1] + b5\n",
        "    y_pred = tf.nn.sigmoid(z_out)\n",
        "\n",
        "    print(f\"z_out (w31*h21 + w32*h22 + b5): {z_out.numpy()}\")\n",
        "    print(f\"y_pred (Sigmoid(z_out)): {y_pred.numpy()}\\n\")\n",
        "\n",
        "    # Calculate Loss (Binary Cross-Entropy)\n",
        "    loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
        "    # Reshape y_pred to match y_true's shape\n",
        "    loss = loss_object(y_true, tf.reshape(y_pred, y_true.shape))\n",
        "\n",
        "    print(f\"Ground Truth (y_true): {y_true.numpy()}\")\n",
        "    print(f\"Predicted Output (y_pred): {y_pred.numpy()}\")\n",
        "    print(f\"Calculated Loss (Binary Cross-Entropy): {loss.numpy()}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDGDOT3bPU20",
        "outputId": "c92345b7-82a6-44a6-8241-337eda69afb1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Forward Pass (TensorFlow) ---\n",
            "z11 (w11*x1 + w12*x2 + b1): 0.5\n",
            "h11 (ReLU(z11)): 0.5\n",
            "z12 (w13*x1 + w14*x2 + b2): 1.100000023841858\n",
            "h12 (ReLU(z12)): 1.100000023841858\n",
            "\n",
            "z21 (w21*h11 + w22*h12 + b3): 0.9100000262260437\n",
            "h21 (ReLU(z21)): 0.9100000262260437\n",
            "z22 (w23*h11 + w24*h12 + b4): 1.2300000190734863\n",
            "h22 (ReLU(z22)): 1.2300000190734863\n",
            "\n",
            "z_out (w31*h21 + w32*h22 + b5): 2.0490000247955322\n",
            "y_pred (Sigmoid(z_out)): 0.8858464956283569\n",
            "\n",
            "Ground Truth (y_true): [[1.]]\n",
            "Predicted Output (y_pred): 0.8858464956283569\n",
            "Calculated Loss (Binary Cross-Entropy): 0.12121159583330154\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Backpropagation**\n"
      ],
      "metadata": {
        "id": "vRnYjlK_Ebt6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute gradients of the loss with respect to all trainable variables\n",
        "gradients = tape.gradient(loss, trainable_variables)\n",
        "\n",
        "# Print gradients for inspection\n",
        "for i, var in enumerate(trainable_variables):\n",
        "    print(f\"dL/d{var.name}: {gradients[i].numpy()}\")\n",
        "\n",
        "# Apply gradients to update weights using a simple SGD optimizer\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
        "\n",
        "# Create a list of (gradient, variable) pairs for the optimizer to apply\n",
        "grads_and_vars = zip(gradients, trainable_variables)\n",
        "\n",
        "# Apply the updates (this is where weights actually change)\n",
        "optimizer.apply_gradients(grads_and_vars)\n",
        "\n",
        "print(\"\\n--- Updated Weights and Biases ---\")\n",
        "for var in trainable_variables:\n",
        "    print(f\"{var.name} (updated): {var.numpy()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6z9I0EaEm06",
        "outputId": "342d061f-24ed-48bd-93cd-0f9db7caa8e1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dL/dw11:0: -0.13127653300762177\n",
            "dL/dw12:0: -0.26255306601524353\n",
            "dL/db1:0: -0.13127653300762177\n",
            "dL/dw13:0: -0.1529656946659088\n",
            "dL/dw14:0: -0.3059313893318176\n",
            "dL/db2:0: -0.1529656946659088\n",
            "dL/dw21:0: -0.0513690747320652\n",
            "dL/dw22:0: -0.11301196366548538\n",
            "dL/db3:0: -0.1027381494641304\n",
            "dL/dw23:0: -0.05707675218582153\n",
            "dL/dw24:0: -0.12556885182857513\n",
            "dL/db4:0: -0.11415350437164307\n",
            "dL/dw31:0: -0.10387969017028809\n",
            "dL/dw32:0: -0.14040881395339966\n",
            "dL/db5:0: -0.11415350437164307\n",
            "\n",
            "--- Updated Weights and Biases ---\n",
            "w11:0 (updated): 0.11312765628099442\n",
            "w12:0 (updated): 0.22625531256198883\n",
            "b1:0 (updated): 0.013127653859555721\n",
            "w13:0 (updated): 0.31529659032821655\n",
            "w14:0 (updated): 0.4305931329727173\n",
            "b2:0 (updated): 0.015296570025384426\n",
            "w21:0 (updated): 0.5051369071006775\n",
            "w22:0 (updated): 0.6113012433052063\n",
            "b3:0 (updated): 0.010273815132677555\n",
            "w23:0 (updated): 0.7057076692581177\n",
            "w24:0 (updated): 0.8125569224357605\n",
            "b4:0 (updated): 0.011415350250899792\n",
            "w31:0 (updated): 0.9103879332542419\n",
            "w32:0 (updated): 1.0140408277511597\n",
            "b5:0 (updated): 0.011415350250899792\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optimizer in Deep Learning\n",
        "An optimizer is an algorithm or method used to adjust the weights and biases of a neural network to minimize the loss function during training. It’s the engine behind model learning—determining how much and in what direction the parameters should change after each training step (based on gradients computed via backpropagation).\n",
        "\n",
        "| Optimizer    | Key Idea                                     | Notes                                           |\n",
        "| ------------ | -------------------------------------------- | ----------------------------------------------- |\n",
        "| **SGD**      | Vanilla stochastic gradient descent          | May converge slowly; sensitive to learning rate |\n",
        "| **Momentum** | Accelerates SGD by adding a velocity term    | Helps escape local minima                       |\n",
        "| **AdaGrad**  | Adapts learning rate for each parameter      | Good for sparse data, but may shrink too much   |\n",
        "| **RMSProp**  | Like AdaGrad but avoids shrinking too fast   | Good for RNNs                                   |\n",
        "| **Adam**     | Combines momentum and adaptive learning rate | Most widely used, generally performs well       |\n"
      ],
      "metadata": {
        "id": "CWudyYyhT5ZJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Implement the Sample Neural Network in PyTorch**"
      ],
      "metadata": {
        "id": "_pUSo7jk8vGl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Inputs\n",
        "x1_val = 1.0\n",
        "x2_val = 2.0\n",
        "X = torch.tensor([[x1_val, x2_val]], dtype=torch.float32) # Input needs to be a tensor with batch dimension\n",
        "\n",
        "# Ground-truth label\n",
        "y_true = torch.tensor([[1.0]], dtype=torch.float32) # Ground truth also needs to be a tensor\n",
        "\n",
        "# Learning Rate\n",
        "learning_rate = 0.1"
      ],
      "metadata": {
        "id": "6U-w_Urg82-B"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Neural Network using nn.Module\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        # Layer 1 (Hidden Layer 1)\n",
        "        self.w11 = nn.Parameter(torch.tensor(0.1, dtype=torch.float32), requires_grad=True)\n",
        "        self.w12 = nn.Parameter(torch.tensor(0.2, dtype=torch.float32), requires_grad=True)\n",
        "        self.b1 = nn.Parameter(torch.tensor(0.0, dtype=torch.float32), requires_grad=True)\n",
        "\n",
        "        self.w13 = nn.Parameter(torch.tensor(0.3, dtype=torch.float32), requires_grad=True)\n",
        "        self.w14 = nn.Parameter(torch.tensor(0.4, dtype=torch.float32), requires_grad=True)\n",
        "        self.b2 = nn.Parameter(torch.tensor(0.0, dtype=torch.float32), requires_grad=True)\n",
        "\n",
        "        # Layer 2 (Hidden Layer 2)\n",
        "        # h21 from (h11, h12)\n",
        "        self.w21 = nn.Parameter(torch.tensor(0.5, dtype=torch.float32), requires_grad=True)\n",
        "        self.w22 = nn.Parameter(torch.tensor(0.6, dtype=torch.float32), requires_grad=True)\n",
        "        self.b3 = nn.Parameter(torch.tensor(0.0, dtype=torch.float32), requires_grad=True)\n",
        "\n",
        "        # h22 from (h11, h12)\n",
        "        self.w23 = nn.Parameter(torch.tensor(0.7, dtype=torch.float32), requires_grad=True)\n",
        "        self.w24 = nn.Parameter(torch.tensor(0.8, dtype=torch.float32), requires_grad=True)\n",
        "        self.b4 = nn.Parameter(torch.tensor(0.0, dtype=torch.float32), requires_grad=True)\n",
        "\n",
        "        # Output Layer\n",
        "        # y_pred from (h21, h22)\n",
        "        self.w31 = nn.Parameter(torch.tensor(0.9, dtype=torch.float32), requires_grad=True)\n",
        "        self.w32 = nn.Parameter(torch.tensor(1.0, dtype=torch.float32), requires_grad=True)\n",
        "        self.b5 = nn.Parameter(torch.tensor(0.0, dtype=torch.float32), requires_grad=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Layer 1\n",
        "        x1 = x[:, 0] # First input feature\n",
        "        x2 = x[:, 1] # Second input feature\n",
        "\n",
        "        # Neuron h11\n",
        "        z11 = self.w11 * x1 + self.w12 * x2 + self.b1\n",
        "        h11 = torch.relu(z11)\n",
        "\n",
        "        # Neuron h12\n",
        "        z12 = self.w13 * x1 + self.w14 * x2 + self.b2\n",
        "        h12 = torch.relu(z12)\n",
        "\n",
        "        H1 = torch.stack([h11, h12], dim=1) # Stack to form a 1x2 tensor\n",
        "\n",
        "        # Layer 2\n",
        "        # Neuron h21\n",
        "        z21 = self.w21 * H1[:, 0] + self.w22 * H1[:, 1] + self.b3\n",
        "        h21 = torch.relu(z21)\n",
        "\n",
        "        # Neuron h22\n",
        "        z22 = self.w23 * H1[:, 0] + self.w24 * H1[:, 1] + self.b4\n",
        "        h22 = torch.relu(z22)\n",
        "\n",
        "        H2 = torch.stack([h21, h22], dim=1) # Stack to form a 1x2 tensor\n",
        "\n",
        "        # Output Layer\n",
        "        z_out = self.w31 * H2[:, 0] + self.w32 * H2[:, 1] + self.b5\n",
        "        y_pred = torch.sigmoid(z_out)\n",
        "\n",
        "        return y_pred, {\n",
        "            'z11': z11, 'h11': h11, 'z12': z12, 'h12': h12,\n",
        "            'z21': z21, 'h21': h21, 'z22': z22, 'h22': h22,\n",
        "            'z_out': z_out\n",
        "        }\n"
      ],
      "metadata": {
        "id": "iWpFydddaYpq"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SimpleNN()\n",
        "\n",
        "# Print initial weights and biases\n",
        "for name, param in model.named_parameters():\n",
        "    print(f\"{name}: {param.item()}\")\n",
        "print(f\"Learning Rate: {learning_rate}\\n\")\n",
        "\n",
        "print(\"--- Forward Pass (PyTorch) ---\")\n",
        "y_pred, intermediates = model(X)\n",
        "\n",
        "print(f\"z11: {intermediates['z11'].item()}\")\n",
        "print(f\"h11 (ReLU(z11)): {intermediates['h11'].item()}\")\n",
        "print(f\"z12: {intermediates['z12'].item()}\")\n",
        "print(f\"h12 (ReLU(z12)): {intermediates['h12'].item()}\\n\")\n",
        "\n",
        "print(f\"z21: {intermediates['z21'].item()}\")\n",
        "print(f\"h21 (ReLU(z21)): {intermediates['h21'].item()}\")\n",
        "print(f\"z22: {intermediates['z22'].item()}\")\n",
        "print(f\"h22 (ReLU(z22)): {intermediates['h22'].item()}\\n\")\n",
        "\n",
        "print(f\"z_out: {intermediates['z_out'].item()}\")\n",
        "print(f\"y_pred (Sigmoid(z_out)): {y_pred.item()}\\n\")\n",
        "\n",
        "# Calculate loss\n",
        "loss_function = nn.BCELoss()\n",
        "# Reshape y_pred to match y_true's shape\n",
        "loss = loss_function(torch.reshape(y_pred, y_true.shape), y_true)\n",
        "\n",
        "print(f\"Ground Truth (y_true): {y_true.item()}\")\n",
        "print(f\"Predicted Output (y_pred): {y_pred.item()}\")\n",
        "print(f\"Initial Loss (Binary Cross-Entropy): {loss.item()}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C4ZXDayNcAfp",
        "outputId": "31394d31-0178-4152-f327-5c4095e23ce4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "w11: 0.10000000149011612\n",
            "w12: 0.20000000298023224\n",
            "b1: 0.0\n",
            "w13: 0.30000001192092896\n",
            "w14: 0.4000000059604645\n",
            "b2: 0.0\n",
            "w21: 0.5\n",
            "w22: 0.6000000238418579\n",
            "b3: 0.0\n",
            "w23: 0.699999988079071\n",
            "w24: 0.800000011920929\n",
            "b4: 0.0\n",
            "w31: 0.8999999761581421\n",
            "w32: 1.0\n",
            "b5: 0.0\n",
            "Learning Rate: 0.1\n",
            "\n",
            "--- Forward Pass (PyTorch) ---\n",
            "z11: 0.5\n",
            "h11 (ReLU(z11)): 0.5\n",
            "z12: 1.100000023841858\n",
            "h12 (ReLU(z12)): 1.100000023841858\n",
            "\n",
            "z21: 0.9100000262260437\n",
            "h21 (ReLU(z21)): 0.9100000262260437\n",
            "z22: 1.2300000190734863\n",
            "h22 (ReLU(z22)): 1.2300000190734863\n",
            "\n",
            "z_out: 2.0490000247955322\n",
            "y_pred (Sigmoid(z_out)): 0.8858465552330017\n",
            "\n",
            "Ground Truth (y_true): 1.0\n",
            "Predicted Output (y_pred): 0.8858465552330017\n",
            "Initial Loss (Binary Cross-Entropy): 0.12121152877807617\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Backpropagation ---\n",
        "print(\"--- Backpropagation (PyTorch) ---\")\n",
        "# Zero the gradients before computing new ones.\n",
        "# This is crucial in PyTorch because gradients accumulate by default.\n",
        "model.zero_grad()\n",
        "\n",
        "# Compute gradients of the loss with respect to all trainable parameters\n",
        "loss.backward()\n",
        "\n",
        "# Print gradients for inspection\n",
        "for name, param in model.named_parameters():\n",
        "    if param.grad is not None:\n",
        "        print(f\"dL/d{name}: {param.grad.item()}\")\n",
        "    else:\n",
        "        print(f\"dL/d{name}: None (no gradient calculated for this parameter)\")\n",
        "\n",
        "print(\"\\n--- Weight Updates (PyTorch Optimizer) ---\")\n",
        "# Initialize an optimizer (Stochastic Gradient Descent)\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Perform a single optimization step (updates weights based on gradients)\n",
        "optimizer.step()\n",
        "\n",
        "print(\"\\n--- Updated Weights and Biases ---\")\n",
        "for name, param in model.named_parameters():\n",
        "    print(f\"{name} (updated): {param.item()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPntpjXEdUTP",
        "outputId": "1faf0993-b7a6-40cf-9c3a-869977107217"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Backpropagation (PyTorch) ---\n",
            "dL/dw11: -0.1312764585018158\n",
            "dL/dw12: -0.2625529170036316\n",
            "dL/db1: -0.1312764585018158\n",
            "dL/dw13: -0.15296562016010284\n",
            "dL/dw14: -0.3059312403202057\n",
            "dL/db2: -0.15296562016010284\n",
            "dL/dw21: -0.05136904865503311\n",
            "dL/dw22: -0.1130119115114212\n",
            "dL/db3: -0.10273809731006622\n",
            "dL/dw23: -0.057076722383499146\n",
            "dL/dw24: -0.12556879222393036\n",
            "dL/db4: -0.11415344476699829\n",
            "dL/dw31: -0.10387963801622391\n",
            "dL/dw32: -0.1404087394475937\n",
            "dL/db5: -0.11415344476699829\n",
            "\n",
            "--- Weight Updates (PyTorch Optimizer) ---\n",
            "\n",
            "--- Updated Weights and Biases ---\n",
            "w11 (updated): 0.11312764883041382\n",
            "w12 (updated): 0.22625529766082764\n",
            "b1 (updated): 0.013127646408975124\n",
            "w13 (updated): 0.31529656052589417\n",
            "w14 (updated): 0.4305931329727173\n",
            "b2 (updated): 0.01529656257480383\n",
            "w21 (updated): 0.5051369071006775\n",
            "w22 (updated): 0.6113012433052063\n",
            "b3 (updated): 0.010273809544742107\n",
            "w23 (updated): 0.7057076692581177\n",
            "w24 (updated): 0.8125568628311157\n",
            "b4 (updated): 0.011415344662964344\n",
            "w31 (updated): 0.9103879332542419\n",
            "w32 (updated): 1.0140408277511597\n",
            "b5 (updated): 0.011415344662964344\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Next Steps:\n",
        "<pre>\n",
        "Input ➝ Hidden Layer(s) ➝ Output\n",
        "   ↓         ↓              ↓\n",
        " Data     Weights      Prediction\n",
        "           +            |\n",
        "         Activation     |\n",
        "           ↓            ↓\n",
        "       Forward Pass → Loss → Backpropagation → Weight Update\n",
        "</pre>\n",
        "\n",
        "- MLP\n",
        "- CNN\n",
        "- YOLO\n",
        "  - ResNET\n",
        "  - VGG"
      ],
      "metadata": {
        "id": "hujcJTSMeC5K"
      }
    }
  ]
}